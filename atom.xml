<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>一只空小白💫</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2024-05-14T04:16:22.181Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Scrapy爬虫框架安装与使用，数据存Mongdb，Mysql数据库</title>
    <link href="http://example.com/2024/05/14/Scrapy%E7%88%AC%E8%99%AB/"/>
    <id>http://example.com/2024/05/14/Scrapy%E7%88%AC%E8%99%AB/</id>
    <published>2024-05-14T03:48:15.000Z</published>
    <updated>2024-05-14T04:16:22.181Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Scrapy爬虫框架安装与使用，数据存Mongdb，Mysql数据库"><a href="#Scrapy爬虫框架安装与使用，数据存Mongdb，Mysql数据库" class="headerlink" title="Scrapy爬虫框架安装与使用，数据存Mongdb，Mysql数据库"></a>Scrapy爬虫框架安装与使用，数据存Mongdb，Mysql数据库</h1><h2 id="开始"><a href="#开始" class="headerlink" title="开始"></a>开始</h2><h2 id="环境搭建"><a href="#环境搭建" class="headerlink" title="环境搭建"></a>环境搭建</h2><p>python环境3.10<br>所需库 mongodb,mysql,scrapy</p><p>pip临时使用清华源，下载速度更快</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs 临时使用">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple mongodb<br>pip install -i https://pypi.tuna.tsinghua.edu.cn/simple mysql<br>pip install -i https://pypi.tuna.tsinghua.edu.cn/simple scrap<br></code></pre></td></tr></table></figure><p><img src="/../images/scrapy/img_1.png" class="lazyload placeholder" data-srcset="/../images/scrapy/img_1.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="alt text" title="Title"></p><h2 id="搭建框架"><a href="#搭建框架" class="headerlink" title="搭建框架"></a>搭建框架</h2><p>新建一个目录存放Scrap爬虫框架，进入到到该目录下，使用命令创建框架</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs angular2html">scrapy startproject pc7k7k #Scrap创建框架<br>cd pc7k7k  #进入到目录<br>scrapy genspider pc_7k7k 7k7k.com  #pc_7k7k爬虫文件名（不可以与目录名相同），7k7k.com是要爬的网站（后续可以改）<br></code></pre></td></tr></table></figure><p><img src="/../images/scrapy/img.png" class="lazyload placeholder" data-srcset="/../images/scrapy/img.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="alt text" title="Title"></p><h2 id="目录结构"><a href="#目录结构" class="headerlink" title="目录结构"></a>目录结构</h2><p><img src="/../images/scrapy/img_2.png" class="lazyload placeholder" data-srcset="/../images/scrapy/img_2.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="alt text" title="Title"></p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scrapy<span class="hljs-selector-class">.cfg</span> ：  项目的配置文件<br>items<span class="hljs-selector-class">.py</span> ：    项目的目标文件<br>pipelines<span class="hljs-selector-class">.py</span> ：项目的管道文件<br>settings<span class="hljs-selector-class">.py</span> ： 项目的设置文件<br>spiders ：     存储爬虫代码目录<br></code></pre></td></tr></table></figure><h3 id="pc-7k7k-py："><a href="#pc-7k7k-py：" class="headerlink" title="pc_7k7k.py："></a>pc_7k7k.py：</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs angular2html">import scrapy<br>from ..items import PachongItem<br><br><br>class Pachong1Spider(scrapy.Spider):<br>    name = &quot;pachong_1&quot;<br>    allowed_domains = [&quot;jntc.nm.cn&quot;]<br>    start_urls = [&quot;http://www.jntc.nm.cn/news/zhxw.htm&quot;]  #综合新闻<br><br>    def parse(self, response):<br>        # 创建一个PachongItem实例来存储提取的数据<br>        item = PachongItem()<br>        item[&#x27;rq&#x27;] = response.xpath(&#x27;//ul//span/text()&#x27;).extract()<br>        item[&#x27;bt&#x27;] = response.xpath(&#x27;//div[@class=&quot;pg-list fr list-right-cont&quot;]//ul//a[@target=&quot;_blank&quot;]/text()&#x27;).extract()<br>        yield item<br><br>        # 查找下一页的链接<br>        next_page = response.xpath(&#x27;//a[@href and contains(text(), &quot;下页&quot;)]/@href&#x27;).get()<br>        if next_page:<br>            next_page_url = response.urljoin(next_page)<br>            yield scrapy.Request(next_page_url, callback=self.parse)  # 递归调用parse来处理下一页<br></code></pre></td></tr></table></figure><h3 id="items-py"><a href="#items-py" class="headerlink" title="items.py:"></a>items.py:</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs angular2html">import scrapy<br><br>class PachongItem(scrapy.Item):<br>    _id = scrapy.Field()<br>    rq = scrapy.Field()<br>    bt = scrapy.Field()<br><br></code></pre></td></tr></table></figure><h3 id="pipelines-py"><a href="#pipelines-py" class="headerlink" title="pipelines.py:"></a>pipelines.py:</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs angular2html">import pymongo<br>import pymysql<br><br>#存Mongodb<br>class MongodbPipeline:<br><br>    def open_spider(self, spider):<br>        self.client = pymongo.MongoClient(host=&quot;127.0.0.1&quot;,port=27017)<br>    def process_item(self, item, spider):<br>        self.client.jnsf.sj.insert_one(item)<br>        return item<br>    def close_spider(self, spider):<br>        self.client.close()<br><br>#存Mysql<br>class MysqlPipeline:<br>    def __init__(self):<br>        self.conn = pymysql.connect(host=&#x27;localhost&#x27;, user=&#x27;root&#x27;, password=&#x27;123456789&#x27;, db=&#x27;jnsf&#x27;,charset=&#x27;utf8mb4&#x27;)<br>        self.cursor = self.conn.cursor()<br>        # 检查表是否存在，如果不存在则创建它<br>        self.create_table_if_not_exists()<br>    def create_table_if_not_exists(self):<br>        create_table_sql = &#x27;&#x27;&#x27;  <br>        CREATE TABLE IF NOT EXISTS jnsf_table (  <br>            id INT AUTO_INCREMENT PRIMARY KEY,  <br>            Publication_date VARCHAR(1000) NOT NULL,  <br>            title VARCHAR(1000) NOT NULL  <br>        )  <br>        &#x27;&#x27;&#x27;<br>        try:<br>            self.cursor.execute(create_table_sql)<br>            self.conn.commit()<br>            print(&quot;创建成功&quot;)<br>        except pymysql.MySQLError as e:<br>            print(f&quot;创建失败 &#123;e&#125;&quot;)<br><br>    def process_item(self, item, spider):<br>        insert_sql = &#x27;INSERT INTO jnsf_table(Publication_date, title) VALUES (%s, %s)&#x27;<br>        for i, title in enumerate(item[&#x27;bt&#x27;]):<br>            self.cursor.execute(insert_sql, (item[&#x27;rq&#x27;][i], item[&#x27;bt&#x27;][i]))<br>        self.conn.commit()<br>        return item<br><br>    def close_spider(self, spider):<br>        self.cursor.close()<br>        self.conn.close()<br></code></pre></td></tr></table></figure><h3 id="settings-py"><a href="#settings-py" class="headerlink" title="settings.py:"></a>settings.py:</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs angular2html">ROBOTSTXT_OBEY = False  <br>DOWNLOAD_DELAY = 1<br>ITEM_PIPELINES = &#123;<br>   &quot;pachong.pipelines.MongodbPipeline&quot;: 300,<br>   &quot;pachong.pipelines.MysqlPipeline&quot;: 400,<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="运行爬虫"><a href="#运行爬虫" class="headerlink" title="运行爬虫"></a>运行爬虫</h2><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs gcode">scrapy crawl pacho<span class="hljs-symbol">ng_1</span>     <span class="hljs-attr"># pachong_1</span>爬虫的名字<br></code></pre></td></tr></table></figure><h2 id="结束"><a href="#结束" class="headerlink" title="结束"></a>结束</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Scrapy爬虫框架安装与使用，数据存Mongdb，Mysql数据库&quot;&gt;&lt;a href=&quot;#Scrapy爬虫框架安装与使用，数据存Mongdb，Mysql数据库&quot; class=&quot;headerlink&quot; title=&quot;Scrapy爬虫框架安装与使用，数据存Mongd</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://example.com/2024/05/13/hello-world/"/>
    <id>http://example.com/2024/05/13/hello-world/</id>
    <published>2024-05-13T13:33:01.323Z</published>
    <updated>2024-05-13T13:33:01.323Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
</feed>
