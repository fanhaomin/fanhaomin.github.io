<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>一只空小白💫</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2024-05-14T13:42:35.310Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Scrapy爬虫框架安装与使用，数据存Mongdb，Mysql数据库</title>
    <link href="http://example.com/2024/05/14/Scrapy%E7%88%AC%E8%99%AB/"/>
    <id>http://example.com/2024/05/14/Scrapy%E7%88%AC%E8%99%AB/</id>
    <published>2024-05-14T03:48:15.000Z</published>
    <updated>2024-05-14T13:42:35.310Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Scrapy爬虫框架安装与使用，数据存Mongdb，Mysql数据库"><a href="#Scrapy爬虫框架安装与使用，数据存Mongdb，Mysql数据库" class="headerlink" title="Scrapy爬虫框架安装与使用，数据存Mongdb，Mysql数据库"></a>Scrapy爬虫框架安装与使用，数据存Mongdb，Mysql数据库</h1><hr><h2 id="1、安装环境"><a href="#1、安装环境" class="headerlink" title="1、安装环境"></a>1、安装环境</h2><p>python环境3.10<br>所需库 mongodb,mysql,scrapy<br>pip临时使用清华源，下载速度更快</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple mongodby</span><br><span class="line">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple mysql</span><br><span class="line">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple scrap</span><br></pre></td></tr></table></figure><p><img src="/../images/scrapy/img_1.png" class="lazyload placeholder" data-srcset="/../images/scrapy/img_1.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><h2 id="2、创建框架"><a href="#2、创建框架" class="headerlink" title="2、创建框架"></a>2、创建框架</h2><p>新建一个目录存放Scrapy爬虫框架，进入到到该目录下，使用命令创建框架</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject pc7k7k #Scrap创建框架</span><br><span class="line">cd pc7k7k  #进入到目录</span><br><span class="line">scrapy genspider pc_7k7k 7k7k.com  #pc_7k7k爬虫文件名（不可以与目录名相同），7k7k.com是要爬的网站（后续可以改）</span><br></pre></td></tr></table></figure><p><img src="/../images/scrapy/img.png" class="lazyload placeholder" data-srcset="/../images/scrapy/img.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><h2 id="3、目录结构"><a href="#3、目录结构" class="headerlink" title="3、目录结构"></a>3、目录结构</h2><p><img src="/../images/scrapy/img_2.png" class="lazyload placeholder" data-srcset="/../images/scrapy/img_2.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scrapy.cfg ：  项目的配置文件</span><br><span class="line">items.py ：    项目的目标文件</span><br><span class="line">pipelines.py ：项目的管道文件</span><br><span class="line">settings.py ： 项目的设置文件</span><br><span class="line">spiders ：     存储爬虫代码目录</span><br></pre></td></tr></table></figure><h2 id="4、编写爬虫"><a href="#4、编写爬虫" class="headerlink" title="4、编写爬虫"></a>4、编写爬虫</h2><h3 id="pc-7k7k-py："><a href="#pc-7k7k-py：" class="headerlink" title="pc_7k7k.py："></a>pc_7k7k.py：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> ..items <span class="keyword">import</span> PachongItem</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Pachong1Spider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&quot;pachong_1&quot;</span>  <span class="comment">#运行爬虫时候的名字</span></span><br><span class="line">    allowed_domains = [<span class="string">&quot;xxx.com&quot;</span>]   </span><br><span class="line">    start_urls = [<span class="string">&quot;http://www.xxx.com/xxx.htm&quot;</span>]  <span class="comment">#要爬的页面网址</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="comment"># 创建一个PachongItem实例来存储提取的数据</span></span><br><span class="line">        item = PachongItem()</span><br><span class="line">        item[<span class="string">&#x27;rq&#x27;</span>] = response.xpath(<span class="string">&#x27;//ul//span/text()&#x27;</span>).extract()</span><br><span class="line">        item[<span class="string">&#x27;bt&#x27;</span>] = response.xpath(<span class="string">&#x27;//div[@class=&quot;pg-list fr list-right-cont&quot;]//ul//a[@target=&quot;_blank&quot;]/text()&#x27;</span>).extract()</span><br><span class="line">        <span class="keyword">yield</span> item</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 查找下一页的链接</span></span><br><span class="line">        next_page = response.xpath(<span class="string">&#x27;//a[@href and contains(text(), &quot;下页&quot;)]/@href&#x27;</span>).get()</span><br><span class="line">        <span class="keyword">if</span> next_page:</span><br><span class="line">            next_page_url = response.urljoin(next_page)</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(next_page_url, callback=self.parse)  <span class="comment"># 递归调用parse来处理下一页</span></span><br></pre></td></tr></table></figure><h3 id="items-py"><a href="#items-py" class="headerlink" title="items.py:"></a>items.py:</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PachongItem</span>(scrapy.Item): </span><br><span class="line">    _<span class="built_in">id</span> = scrapy.Field()</span><br><span class="line">    rq = scrapy.Field()</span><br><span class="line">    bt = scrapy.Field()</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="pipelines-py"><a href="#pipelines-py" class="headerlink" title="pipelines.py:"></a>pipelines.py:</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line"><span class="comment">#存Mongodb</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MongodbPipeline</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">open_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        self.client = pymongo.MongoClient(host=<span class="string">&quot;127.0.0.1&quot;</span>,port=<span class="number">27017</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        self.client.jnsf.sj.insert_one(item)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        self.client.close()</span><br><span class="line"></span><br><span class="line"><span class="comment">#存Mysql</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MysqlPipeline</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.conn = pymysql.connect(host=<span class="string">&#x27;localhost&#x27;</span>, user=<span class="string">&#x27;root&#x27;</span>, password=<span class="string">&#x27;123456789&#x27;</span>, db=<span class="string">&#x27;jnsf&#x27;</span>,charset=<span class="string">&#x27;utf8mb4&#x27;</span>)</span><br><span class="line">        self.cursor = self.conn.cursor()</span><br><span class="line">        <span class="comment"># 检查表是否存在，如果不存在则创建它</span></span><br><span class="line">        self.create_table_if_not_exists()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">create_table_if_not_exists</span>(<span class="params">self</span>):</span><br><span class="line">        create_table_sql = <span class="string">&#x27;&#x27;&#x27;  </span></span><br><span class="line"><span class="string">        CREATE TABLE IF NOT EXISTS jnsf_table (  </span></span><br><span class="line"><span class="string">            id INT AUTO_INCREMENT PRIMARY KEY,  </span></span><br><span class="line"><span class="string">            Publication_date VARCHAR(1000) NOT NULL,  </span></span><br><span class="line"><span class="string">            title VARCHAR(1000) NOT NULL  </span></span><br><span class="line"><span class="string">        )  </span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            self.cursor.execute(create_table_sql)</span><br><span class="line">            self.conn.commit()</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;创建成功&quot;</span>)</span><br><span class="line">        <span class="keyword">except</span> pymysql.MySQLError <span class="keyword">as</span> e:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;创建失败 <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        insert_sql = <span class="string">&#x27;INSERT INTO jnsf_table(Publication_date, title) VALUES (%s, %s)&#x27;</span></span><br><span class="line">        <span class="keyword">for</span> i, title <span class="keyword">in</span> <span class="built_in">enumerate</span>(item[<span class="string">&#x27;bt&#x27;</span>]):</span><br><span class="line">            self.cursor.execute(insert_sql, (item[<span class="string">&#x27;rq&#x27;</span>][i], item[<span class="string">&#x27;bt&#x27;</span>][i]))</span><br><span class="line">        self.conn.commit()</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        self.cursor.close()</span><br><span class="line">        self.conn.close()</span><br></pre></td></tr></table></figure><h3 id="settings-py"><a href="#settings-py" class="headerlink" title="settings.py:"></a>settings.py:</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span>  </span><br><span class="line">DOWNLOAD_DELAY = <span class="number">1</span></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">&quot;pachong.pipelines.MongodbPipeline&quot;</span>: <span class="number">300</span>,</span><br><span class="line">   <span class="string">&quot;pachong.pipelines.MysqlPipeline&quot;</span>: <span class="number">400</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="5、运行爬虫"><a href="#5、运行爬虫" class="headerlink" title="5、运行爬虫"></a>5、运行爬虫</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl pachong_1     <span class="comment"># pachong_1爬虫的名字</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Scrapy爬虫框架安装与使用，数据存Mongdb，Mysql数据库&quot;&gt;&lt;a href=&quot;#Scrapy爬虫框架安装与使用，数据存Mongdb，Mysql数据库&quot; class=&quot;headerlink&quot; title=&quot;Scrapy爬虫框架安装与使用，数据存Mongd</summary>
      
    
    
    
    
    <category term="Scrapy" scheme="http://example.com/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>数据采集与预处理实验</title>
    <link href="http://example.com/2024/05/14/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/"/>
    <id>http://example.com/2024/05/14/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/</id>
    <published>2024-05-14T03:48:15.000Z</published>
    <updated>2024-05-14T13:42:18.930Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数据采集与预处理实验"><a href="#数据采集与预处理实验" class="headerlink" title="数据采集与预处理实验"></a>数据采集与预处理实验</h1><hr><h1 id="要求"><a href="#要求" class="headerlink" title="要求"></a>要求</h1><p>五个小实验文档提交（前四个个人作业，第五个小组作业。需提交配套文档，文档模板如下）<br>1.百度新闻的爬取&amp;自由扩展爬取<br>2.bs4重新爬取百度新闻<br>3.飞卢小说网爬取与存储数据库<br>4.设计一个数据库系统，要求至少5张表以上，用workbench作图，属性完整，必要的连接完整<br>5.Pyspider的安装与运行</p><hr><p><code>提示：以下是全部实验报告下载地址，可供参考</code><br><strong>（实验四、实验五只有实验报告）实验报告下载地址: <a href="https://pan.baidu.com/s/1BSYLksyIBvvHSUHW3SJ31w?pwd=kbcx">百度网盘</a></strong></p><hr><h1 id="1、百度新闻的爬取-自由扩展爬取"><a href="#1、百度新闻的爬取-自由扩展爬取" class="headerlink" title="1、百度新闻的爬取&amp;自由扩展爬取"></a>1、百度新闻的爬取&amp;自由扩展爬取</h1><p>代码如下（示例）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入需要的模块</span></span><br><span class="line"><span class="keyword">import</span> urllib.request  <span class="comment"># 用于从URL获取数据</span></span><br><span class="line"><span class="keyword">import</span> re  <span class="comment"># 用于正则表达式匹配</span></span><br><span class="line"><span class="keyword">import</span> datetime  <span class="comment"># 用于处理日期和时间</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义两个URL，分别对应新闻和视频网站</span></span><br><span class="line">url1 = <span class="string">&#x27;https://news.baidu.com/&#x27;</span></span><br><span class="line">url2 = <span class="string">&#x27;https://v.xiaodutv.com/&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用urllib.request模块从URL中获取内容，并解码为utf-8格式</span></span><br><span class="line">content1 = urllib.request.urlopen(url1).read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">content2 = urllib.request.urlopen(url2).read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义两个正则表达式模式，用于匹配新闻和视频网站的特定内容</span></span><br><span class="line"><span class="comment"># pattern1用于匹配新闻网站的热点新闻标题</span></span><br><span class="line">pattern1 = re.<span class="built_in">compile</span>(<span class="string">&#x27;&lt;li class=&quot;hdline.*?&lt;strong&gt;.*?&lt;a.*?&gt;(.*?)&lt;/a&gt;.*?strong&gt;&#x27;</span>, re.S)</span><br><span class="line"><span class="comment"># pattern2用于匹配视频网站的热点视频标题</span></span><br><span class="line">pattern2 = re.<span class="built_in">compile</span>(<span class="string">&quot;&lt;li class=&#x27;poste.*?&lt;a.*?&lt;img.*?&lt;p.*?&gt;(.*?)&lt;/p&gt;&quot;</span>, re.S)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用正则表达式从内容中提取匹配的信息</span></span><br><span class="line">hotNews1 = re.findall(pattern1, content1)</span><br><span class="line">hotNews2 = re.findall(pattern2, content2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印提取的热点新闻</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> hotNews1:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;热点新闻：&quot;</span>, i)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&quot;</span>)</span><br><span class="line"><span class="comment"># 打印提取的热点视频</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> hotNews2:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;热点视频:&quot;</span>, i)</span><br><span class="line"><span class="comment"># 打印当前的时间</span></span><br><span class="line"><span class="built_in">print</span>(datetime.datetime.now())</span><br></pre></td></tr></table></figure><h1 id="2、bs4重新爬取百度新闻"><a href="#2、bs4重新爬取百度新闻" class="headerlink" title="2、bs4重新爬取百度新闻"></a>2、bs4重新爬取百度新闻</h1><p>代码如下（示例）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup  <span class="comment"># 导入BeautifulSoup库，用于解析HTML内容</span></span><br><span class="line"><span class="keyword">import</span> requests  <span class="comment"># 导入requests库，用于发送HTTP请求</span></span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://news.baidu.com&#x27;</span>  <span class="comment"># 定义要爬取的网页URL</span></span><br><span class="line"><span class="comment"># 使用requests库发送GET请求，获取网页内容</span></span><br><span class="line">res = requests.get(url)</span><br><span class="line"><span class="comment"># 使用BeautifulSoup解析网页内容</span></span><br><span class="line">soup = BeautifulSoup(res.text, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个函数，用于移除字符串中的空行</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">remove_empty_lines</span>(<span class="params">s</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;\n&#x27;</span>.join([line <span class="keyword">for</span> line <span class="keyword">in</span> s.splitlines() <span class="keyword">if</span> line.strip()])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;热点新闻：\n&#x27;</span>)  <span class="comment"># 打印标题，表示接下来输出的是热点新闻</span></span><br><span class="line"><span class="comment"># 循环6次，尝试查找不同类名的热点新闻</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>):</span><br><span class="line">    news_list = soup.find_all(class_=<span class="string">&#x27;hdline&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i))  <span class="comment"># 使用find_all方法查找类名为&#x27;hdline0&#x27;到&#x27;hdline5&#x27;的元素</span></span><br><span class="line">    <span class="keyword">for</span> news <span class="keyword">in</span> news_list:  <span class="comment"># 遍历找到的所有新闻元素</span></span><br><span class="line">        <span class="built_in">print</span>(remove_empty_lines(news.get_text()))  <span class="comment"># 打印新闻内容，并移除空行</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\n其他新闻：\n&#x27;</span>)  <span class="comment"># 打印标题，表示接下来输出的是其他新闻</span></span><br><span class="line"><span class="comment"># 查找类名为&#x27;ulist focuslistnews&#x27;的元素，这些元素通常包含其他新闻</span></span><br><span class="line">news_list1 = soup.find_all(class_=<span class="string">&#x27;ulist focuslistnews&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> news <span class="keyword">in</span> news_list1:  <span class="comment"># 遍历找到的所有其他新闻元素</span></span><br><span class="line">    <span class="built_in">print</span>(remove_empty_lines(news.get_text()))  <span class="comment"># 打印新闻内容，并移除空行</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;热搜新闻词：\n&#x27;</span>)  <span class="comment"># 打印标题，表示接下来输出的是热搜新闻词</span></span><br><span class="line"><span class="comment"># 查找类名为&#x27;bd&#x27;的元素，这些元素通常包含热搜新闻词</span></span><br><span class="line">news_list2 = soup.find_all(class_=<span class="string">&#x27;bd&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> news <span class="keyword">in</span> news_list2:  <span class="comment"># 遍历找到的所有热搜新闻词元素</span></span><br><span class="line">    <span class="built_in">print</span>(remove_empty_lines(news.get_text()))  <span class="comment"># 打印热搜新闻词内容，并移除空行</span></span><br></pre></td></tr></table></figure><h1 id="3、卢小说网爬取与存储数据库"><a href="#3、卢小说网爬取与存储数据库" class="headerlink" title="3、卢小说网爬取与存储数据库"></a>3、卢小说网爬取与存储数据库</h1><p>代码如下（示例）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line"><span class="comment">#10本小说网址</span></span><br><span class="line">url = [ <span class="string">&#x27;https://b.faloo.com/1409514.html&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;https://b.faloo.com/1406411.html&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;https://b.faloo.com/1378270.html&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;https://b.faloo.com/1406675.html&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;https://b.faloo.com/1408380.html&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;https://b.faloo.com/1236995.html&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;https://b.faloo.com/1405654.html&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;https://b.faloo.com/1376693.html&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;https://b.faloo.com/1410749.html&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;https://b.faloo.com/671060.html&#x27;</span></span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">db = pymysql.connect(</span><br><span class="line">    host=<span class="string">&#x27;127.0.0.1&#x27;</span>,      <span class="comment"># 外网/内网地址</span></span><br><span class="line">    user=<span class="string">&#x27;root&#x27;</span>,           <span class="comment"># 账号</span></span><br><span class="line">    password=<span class="string">&#x27;123456789&#x27;</span>,  <span class="comment"># 密码</span></span><br><span class="line">    db=<span class="string">&#x27;xiaoshuo&#x27;</span>,         <span class="comment"># database名称</span></span><br><span class="line">    charset=<span class="string">&#x27;utf8&#x27;</span>         <span class="comment"># 编码格式</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拿到游标</span></span><br><span class="line">cursor = db.cursor()</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_url</span>(<span class="params">url</span>):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = requests.get(url)</span><br><span class="line">        response.raise_for_status()</span><br><span class="line">        soup = BeautifulSoup(response.text, <span class="string">&#x27;html.parser&#x27;</span>)</span><br><span class="line">        Topic = soup.find(<span class="string">&#x27;h1&#x27;</span>, class_=<span class="string">&#x27;fs23&#x27;</span>).text</span><br><span class="line">        divs = soup.find_all(<span class="string">&#x27;div&#x27;</span>, class_=<span class="string">&#x27;DivTd3&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> divs, Topic</span><br><span class="line">    <span class="keyword">except</span> requests.HTTPError <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;获取网页错误：<span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> []</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_content</span>(<span class="params">divs, path, a</span>):</span><br><span class="line">        <span class="keyword">for</span> div <span class="keyword">in</span> divs:</span><br><span class="line">            url = <span class="string">&#x27;https:&#x27;</span> + div.a[<span class="string">&#x27;href&#x27;</span>]</span><br><span class="line">            chapter_name = div.text</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                rq = requests.get(url)</span><br><span class="line">                rq.raise_for_status()</span><br><span class="line">                soup1 = BeautifulSoup(rq.text, <span class="string">&#x27;html.parser&#x27;</span>)</span><br><span class="line">                chapter_content = soup1.find(name=<span class="string">&#x27;div&#x27;</span>, class_=a).text</span><br><span class="line">                <span class="comment"># 存到数据库</span></span><br><span class="line">                sql = <span class="string">&quot;INSERT INTO xiao_shuo (书名,章节,内容) VALUES (%s,%s,%s)&quot;</span></span><br><span class="line">                val = (path, chapter_name, chapter_content)  <span class="comment"># 这里替换为你要插入的实际值</span></span><br><span class="line">                cursor.execute(sql, val)</span><br><span class="line">                db.commit()</span><br><span class="line">            <span class="keyword">except</span> requests.HTTPError <span class="keyword">as</span> e:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;访问章节页面失败：&#x27;<span class="subst">&#123;e&#125;</span>&#x27;&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;path&#125;</span>------------------全部章节已下载到数据库！&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    url1, Topic = get_url(url[i])</span><br><span class="line">    get_content(url1, Topic, <span class="string">&#x27;noveContent&#x27;</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;数据采集与预处理实验&quot;&gt;&lt;a href=&quot;#数据采集与预处理实验&quot; class=&quot;headerlink&quot; title=&quot;数据采集与预处理实验&quot;&gt;&lt;/a&gt;数据采集与预处理实验&lt;/h1&gt;&lt;hr&gt;
&lt;h1 id=&quot;要求&quot;&gt;&lt;a href=&quot;#要求&quot; class=&quot;hea</summary>
      
    
    
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://example.com/2024/05/13/arduino/"/>
    <id>http://example.com/2024/05/13/arduino/</id>
    <published>2024-05-13T13:33:01.323Z</published>
    <updated>2024-05-14T12:25:19.630Z</updated>
    
    
    
    
    
  </entry>
  
</feed>
